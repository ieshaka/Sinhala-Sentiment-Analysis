{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/output.csv', sep='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(keep='first')\n",
    "data = data.dropna()\n",
    "data['Sentiment'] = data['Sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data - remove punctuation from every newsgroup text\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.HASHTAG)\n",
    "\n",
    "cleaned_tweets = []\n",
    "for line in data['Tweet']:\n",
    "    line = line.strip()\n",
    "    line = p.clean(line)\n",
    "    line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
    "    cleaned_tweets.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet']= cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for line in data['cleaned_tweet']:\n",
    "    line = line.strip()\n",
    "    for word in line.split():\n",
    "        if word not in words:\n",
    "            words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = words[369:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "with open('data/Suffixes-413.txt','r',encoding = 'UTF-8')as l:\n",
    "    for line in l:\n",
    "        if line!='\\n':\n",
    "            vv=line.strip()\n",
    "            suffixes.append(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = []\n",
    "stem.append(words_to_stem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in words_to_stem[1:]:\n",
    "#     flag = False\n",
    "#     for suffix in suffixes:\n",
    "#         if word.endswith(suffix):\n",
    "#             rest = word[:-(len(suffix))]\n",
    "#             stem.append(rest)\n",
    "#             flag = True\n",
    "#             break\n",
    "#     if flag == False:\n",
    "#         stem.append(word)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_to_stem)):\n",
    "    flag = False\n",
    "    if i >0:\n",
    "        if words_to_stem[i].startswith(words_to_stem[i-1]):\n",
    "            for suffix in suffixes:\n",
    "                if words_to_stem[i].endswith(suffix):\n",
    "                    stem.append(stem[i-1])\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag == False:\n",
    "                stem.append(words_to_stem[i])\n",
    "                    \n",
    "        else:\n",
    "            stem.append(words_to_stem[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16370, 16370)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_stem),len(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_df = pd.DataFrame({'word':words_to_stem ,'stem':stem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_df.to_csv('stem.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = open('all_data.txt', 'a',  encoding = 'UTF-8')\n",
    "#print(stem)\n",
    "for i in data['cleaned_tweet']:\n",
    "    q.writelines(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "with open('all_data.txt','r',encoding = 'UTF-8')as input_file:\n",
    "    count = Counter(word for line in input_file\n",
    "                         for word in line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('නෑ', 1006), ('නම්', 538), ('මේ', 400), ('එක', 391), ('කියලා', 344), ('මට', 287), ('දැන්', 276), ('ඒ', 266), ('මම', 247), ('ගැන', 246), ('එකක්', 242), ('බන්', 227), ('වගේ', 225), ('මන්', 204), ('තමා', 203), ('කරන්න', 200), ('අද', 198), ('ඔව්', 194), ('වෙලා', 186), ('අපි', 180), ('හරි', 180), ('ඒක', 179), ('ඇති', 179), ('නේ', 173), ('නැති', 166), ('නිසා', 166), ('එපා', 159), ('ඉන්න', 158), ('යන්න', 147), ('ඔබ', 146), ('වෙයි', 144), ('එහෙම', 144), ('ගන්න', 143), ('වෙන්න', 143), ('එකේ', 134), ('බෑ', 134), ('වෙන', 131), ('තියෙනවා', 129), ('මං', 127), ('මගේ', 126), ('එක්ක', 119), ('උඹ', 113), ('වඩා', 112), ('ඇයි', 112), ('කරන', 108), ('උන්', 107), ('තාම', 106), ('ඔය', 105), ('කියන්නේ', 104), ('ඕන', 104), ('වැඩ', 103), ('මේක', 102), ('ඉතින්', 101), ('ඕනේ', 101), ('ඔයා', 100), ('ද', 98), ('දන්නේ', 98), ('අය', 97), ('ආදරේ', 96), ('වල', 96), ('නැද්ද', 96), ('බලන්න', 95), ('ගිය', 91), ('අනේ', 89), ('කතා', 89), ('වෙනවා', 88), ('අපිට', 88), ('එන්න', 88), ('තවත්', 87), ('වලට', 86), ('නෙමෙයි', 85), ('ජනපති', 84), ('ඉන්නේ', 83), ('තියෙන', 82), ('හා', 82), ('තියෙන්නේ', 82), ('ගිහින්', 81), ('ඒවා', 80), ('කරයි', 80), ('අපේ', 79), ('ඉන්නවා', 79), ('කරලා', 78), ('විතරයි', 76), ('කියයි', 75), ('වැඩි', 75), ('දෙන්න', 74), ('යනවා', 74), ('ගෙදර', 73), ('වෙන්නේ', 72), ('කියන්න', 71), ('පුළුවන්', 71), ('කළ', 70), ('යන', 70), ('මහ', 70), ('හෙට', 69), ('නැතුව', 67), ('ආදරය', 66), ('ඔබට', 66), ('මහින්ද', 66), ('එකට', 65)]\n"
     ]
    }
   ],
   "source": [
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_test1",
   "language": "python",
   "name": "msc_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
